{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dbamman/nlp22/blob/main/HW3/HW_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlFS_v7TpRHe"
   },
   "source": [
    "# Homework 3: Pytorch and CNNs\n",
    "\n",
    "In this homework, you will begin exploring Pytorch, a neural network library that will be used throughout the remainder of the semester.  This homework will focus on implementing a bag-of-words logistic regression and a convolutional neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wyY0yl1Jpf2o"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "from collections import Counter\n",
    "\n",
    "#Sets random seeds for reproducibility\n",
    "seed=159259\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s7DVjxeq_-OF",
    "outputId": "2664c1c3-020f-40ae-ba6c-177d4e1f6a5a"
   },
   "outputs": [],
   "source": [
    "!python -m nltk.downloader punkt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlYfHZwlQXA_"
   },
   "source": [
    "When looking up pytorch documentation, it may be useful to know which version of torch you are running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qUdEHON5lybF",
    "outputId": "73d955cb-3969-49fc-dfaa-1174f574907e"
   },
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xrbyc1flKp_"
   },
   "source": [
    "# **IMPORTANT**: GPU is not enabled by default\n",
    "\n",
    "You must switch runtime environments if your output of the next block of code has an error saying \"ValueError: Expected a cuda device, but got: cpu\"\n",
    "\n",
    "Go to Runtime > Change runtime type > Hardware accelerator > GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRy4VWrvkCP6",
    "outputId": "6ed36e66-5775-4389-8b04-d879fcbfa081"
   },
   "outputs": [],
   "source": [
    "device = torch.cuda.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyARzkPKmUlR"
   },
   "source": [
    "# Data Processing\n",
    "\n",
    "Let's begin by loading our datasets and the 50-dimensional GLoVE word embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l_ZZQsGwH5vj"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW3/train.txt\n",
    "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW3/dev.txt\n",
    "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW3/glove.6B.50d.50K.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vC5tWWn2mWhH"
   },
   "outputs": [],
   "source": [
    "trainingFile = \"train.txt\"\n",
    "devFile = \"dev.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_vLcPzzIxDw"
   },
   "outputs": [],
   "source": [
    "labels = {'pos': 0, 'neg': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNb4H1auI4lA"
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, xType, batch_size=12):\n",
    "    batches_x=[]\n",
    "    batches_y=[]\n",
    "    for i in range(0, len(x), batch_size):\n",
    "        batches_x.append(xType(x[i:i+batch_size]))\n",
    "        batches_y.append(torch.LongTensor(y[i:i+batch_size]))\n",
    "    \n",
    "    return batches_x, batches_y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnIbufFHlYSx"
   },
   "outputs": [],
   "source": [
    "PAD_INDEX = 0             # reserved for padding words\n",
    "UNKNOWN_INDEX = 1         # reserved for unknown words\n",
    "SEP_INDEX = 2\n",
    "\n",
    "data_lens = []\n",
    "\n",
    "def read_embeddings(filename, vocab_size=50000):\n",
    "  \"\"\"\n",
    "  Utility function, loads in the `vocab_size` most common embeddings from `filename`\n",
    "  \n",
    "  Arguments:\n",
    "  - filename:     path to file\n",
    "                  automatically infers correct embedding dimension from filename\n",
    "  - vocab_size:   maximum number of embeddings to load\n",
    "\n",
    "  Returns \n",
    "  - embeddings:   torch.FloatTensor matrix of size (vocab_size x word_embedding_dim)\n",
    "  - vocab:        dictionary mapping word (str) to index (int) in embedding matrix\n",
    "  \"\"\"\n",
    "\n",
    "  # get the embedding size from the first embedding\n",
    "  with open(filename, encoding=\"utf-8\") as file:\n",
    "    word_embedding_dim = len(file.readline().split(\" \")) - 1\n",
    "\n",
    "  vocab = {}\n",
    "\n",
    "  embeddings = np.zeros((vocab_size, word_embedding_dim))\n",
    "  with open(filename, encoding=\"utf-8\") as file:\n",
    "    for idx, line in enumerate(file):\n",
    "\n",
    "      if idx + 2 >= vocab_size:\n",
    "        break\n",
    "\n",
    "      cols = line.rstrip().split(\" \")\n",
    "      val = np.array(cols[1:])\n",
    "      word = cols[0]\n",
    "      embeddings[idx + 2] = val\n",
    "      vocab[word] = idx + 2\n",
    "  \n",
    "  # a FloatTensor is a multidimensional matrix\n",
    "  # that contains 32-bit floats in every entry\n",
    "  # https://pytorch.org/docs/stable/tensors.html\n",
    "  return torch.FloatTensor(embeddings), vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrBHMiLPIOKB"
   },
   "source": [
    "# Logistic regression\n",
    "\n",
    "First, let's code up logistic regression in pytorch so you can see how the general framework works, and also get a sense of its performance that we can compare a CNN against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "392D8YLfI_K3"
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionClassifier(nn.Module):\n",
    "\n",
    "   def __init__(self, input_dim, output_dim):\n",
    "      super().__init__()\n",
    "      self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    " \n",
    "    \n",
    "   def forward(self, input): \n",
    "      x1 = self.linear(input)\n",
    "      return x1\n",
    "\n",
    "   def evaluate(self, x, y):\n",
    "\n",
    "      self.eval()\n",
    "      corr = 0.\n",
    "      total = 0.\n",
    "      with torch.no_grad():\n",
    "        for x, y in zip(x, y):\n",
    "          y_preds=self.forward(x)\n",
    "          for idx, y_pred in enumerate(y_preds):\n",
    "              prediction=torch.argmax(y_pred)\n",
    "              if prediction == y[idx]:\n",
    "                corr += 1.\n",
    "              total+=1                          \n",
    "      return corr/total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgaDKtOrc10l"
   },
   "source": [
    "## Example: Average Embedding Representation\n",
    "Let's train a logistic regression classifier where the input is the average GLoVE embedding for all words in a review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YgU4027luO3"
   },
   "outputs": [],
   "source": [
    "def read_glove_data(filename, vocab, embs):\n",
    "    data=[]\n",
    "    data_labels=[]\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            avg_emb=np.zeros(50)\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            idd = cols[0]\n",
    "            label = cols[1]\n",
    "            review = cols[2]\n",
    "            words=nltk.word_tokenize(review)\n",
    "            avg_counter = 0.\n",
    "            for word in words:\n",
    "                word=word.lower()\n",
    "                if word in glove_vocab:\n",
    "                    avg_emb += embs[glove_vocab[word]].numpy()\n",
    "                    avg_counter += 1.\n",
    "            avg_emb /= avg_counter\n",
    "\n",
    "            data.append(avg_emb)\n",
    "            data_labels.append(labels[label])\n",
    "    return data, data_labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYb1iVsqb0Le"
   },
   "outputs": [],
   "source": [
    "embs, glove_vocab = read_embeddings(\"glove.6B.50d.50K.txt\")\n",
    "avg_train_x, avg_train_y=read_glove_data(trainingFile, glove_vocab, embs)\n",
    "avg_dev_x, avg_dev_y=read_glove_data(devFile, glove_vocab, embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FeYYf7-c01Z"
   },
   "outputs": [],
   "source": [
    "avg_trainX, avg_trainY=get_batches(avg_train_x, avg_train_y, xType=torch.FloatTensor)\n",
    "avg_devX, avg_devY=get_batches(avg_dev_x, avg_dev_y, xType=torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Duzn0vCrdR5X",
    "outputId": "973c1389-1f4d-4fc8-c760-66b5bb1ebf97"
   },
   "outputs": [],
   "source": [
    "logreg=LogisticRegressionClassifier(50, len(labels))\n",
    "optimizer = torch.optim.Adam(logreg.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "losses = []\n",
    "cross_entropy=nn.CrossEntropyLoss()\n",
    "\n",
    "num_labels=len(labels)\n",
    "\n",
    "patience=10\n",
    "maxDevAccuracy=0\n",
    "patienceCounter=0\n",
    "\n",
    "for epoch in range(200):\n",
    "    logreg.train()\n",
    "    \n",
    "    for x, y in zip(avg_trainX, avg_trainY):\n",
    "        y_pred=logreg.forward(x)\n",
    "        loss = cross_entropy(y_pred.view(-1, num_labels), y.view(-1))\n",
    "        losses.append(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    dev_accuracy=logreg.evaluate(avg_devX, avg_devY)\n",
    "    \n",
    "    # check if the dev accuracy is the best seen so far\n",
    "    if dev_accuracy > maxDevAccuracy:\n",
    "        maxDevAccuracy=dev_accuracy\n",
    "        patienceCounter=0\n",
    "    \n",
    "    patienceCounter+=1\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
    "    if patienceCounter >= patience:\n",
    "        print(\"Stopping training; no improvement on dev data after %s epochs\" % patience)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObjO1BiXc_nY"
   },
   "source": [
    "# Deliverable 1. BOW Representation\n",
    "Your last homework used sklearn for logistic regression classification using a bag-of-words representation. Here you'll do the same thing, but in pytorch.  Fill in a bag-of-words implementation into read_bow_data() to see how the logistic classifier model works with this different featurization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0r3LM0mKDxVP"
   },
   "outputs": [],
   "source": [
    "# This function creates a unigram vocabulary from the most frequent 10K words in the training data\n",
    "def get_vocab(filename, max_words=10000):\n",
    "    unigram_counts=Counter()\n",
    "    with open(filename) as file:    \n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            idd = cols[0]\n",
    "            label = cols[1]\n",
    "            review = cols[2]\n",
    "            words=nltk.word_tokenize(review)\n",
    "\n",
    "            for word in words:\n",
    "                word=word.lower()\n",
    "                unigram_counts[word]+=1\n",
    "\n",
    "    vocab={}\n",
    "    for k,v in unigram_counts.most_common(max_words):\n",
    "        vocab[k]=len(vocab)\n",
    "    return vocab\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huWSY2FNlqrF"
   },
   "outputs": [],
   "source": [
    "def read_bow_data(filename, vocab):\n",
    "    data=[]\n",
    "    data_labels=[]\n",
    "    with open(filename) as file:\n",
    "        for line in file:\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            idd = cols[0]\n",
    "            label = cols[1]\n",
    "            review = cols[2]\n",
    "            bow=np.zeros(len(vocab))\n",
    "\n",
    "            '''\n",
    "            Add your bow code here to store the featurization in the bow variable. \n",
    "            \n",
    "            '''\n",
    "\n",
    "            data.append(bow)\n",
    "\n",
    "            data_labels.append(labels[label])\n",
    "    return data, data_labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tc9ZGJvWImkA"
   },
   "outputs": [],
   "source": [
    "bow_vocab=get_vocab(trainingFile)\n",
    "bow_train_x, bow_train_y=read_bow_data(trainingFile, bow_vocab)\n",
    "bow_dev_x, bow_dev_y=read_bow_data(devFile, bow_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LFucMsZII8Hb"
   },
   "outputs": [],
   "source": [
    "bow_trainX, bow_trainY=get_batches(bow_train_x, bow_train_y, xType=torch.FloatTensor)\n",
    "bow_devX, bow_devY=get_batches(bow_dev_x, bow_dev_y, xType=torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "byr4SJB1JCDO",
    "outputId": "64045707-fe6e-459d-9869-4bc248bda12d"
   },
   "outputs": [],
   "source": [
    "logreg=LogisticRegressionClassifier(len(bow_vocab), len(labels))\n",
    "optimizer = torch.optim.Adam(logreg.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "losses = []\n",
    "cross_entropy=nn.CrossEntropyLoss()\n",
    "best_dev_acc = 0.\n",
    "\n",
    "num_labels=len(labels)\n",
    "\n",
    "patience=10\n",
    "patienceCounter=0\n",
    "\n",
    "for epoch in range(200):\n",
    "    for x, y in zip(bow_trainX, bow_trainY):\n",
    "        y_pred=logreg.forward(x)\n",
    "        loss = cross_entropy(y_pred.view(-1, num_labels), y.view(-1))\n",
    "        losses.append(loss)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    dev_accuracy=logreg.evaluate(bow_devX, bow_devY)\n",
    "            \n",
    "    if epoch % 5 == 0:\n",
    "        print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
    "\n",
    "    # check if the dev accuracy is the best seen so far; save the model if so\n",
    "    if dev_accuracy > best_dev_acc:\n",
    "      torch.save(logreg.state_dict(), 'best-bowmodel-parameters.pt')\n",
    "      best_dev_acc = dev_accuracy\n",
    "      patienceCounter=0\n",
    "\n",
    "    patienceCounter+=1\n",
    "    if patienceCounter >= patience:\n",
    "        print(\"Stopping training; no improvement on dev data after %s epochs\" % patience)\n",
    "        break\n",
    "    \n",
    "logreg.load_state_dict(torch.load('best-bowmodel-parameters.pt'))\n",
    "print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbHrmE4jJQrT"
   },
   "source": [
    "# Deliverable 2. CNN \n",
    "\n",
    "Now let's create our CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YhST7BOJPoG"
   },
   "outputs": [],
   "source": [
    "def read_data(filename, vocab, labels):\n",
    "    \"\"\"\n",
    "    :param filename: the name of the file\n",
    "    :return: list of tuple ([word index list], label)\n",
    "    as input for the forward and backward function\n",
    "    \"\"\"    \n",
    "    data = []\n",
    "    data_labels = []\n",
    "    file = open(filename)\n",
    "    for line in file:\n",
    "        cols = line.split(\"\\t\")\n",
    "        idd = cols[0]\n",
    "        label = cols[1]\n",
    "        review = cols[2]\n",
    "        w_int = []\n",
    "        for w in nltk.word_tokenize(review.lower()):\n",
    "            if w in vocab:\n",
    "                w_int.append(vocab[w])\n",
    "            else:\n",
    "                w_int.append(UNKNOWN_INDEX)\n",
    "        data_lens.append(len(w_int))\n",
    "        if len(w_int) < 549:\n",
    "            w_int.extend([PAD_INDEX] * (549 - len(w_int)))\n",
    "        if len(w_int) < 550:\n",
    "          data.append((w_int))\n",
    "          data_labels.append(labels[label])\n",
    "    file.close()\n",
    "    return data, data_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9sB60ratJZvB"
   },
   "outputs": [],
   "source": [
    "embs, cnn_vocab = read_embeddings(\"glove.6B.50d.50K.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hStl2tmiJesV"
   },
   "outputs": [],
   "source": [
    "cnn_train_x, cnn_train_y = read_data(trainingFile, cnn_vocab, labels)\n",
    "cnn_dev_x, cnn_dev_y = read_data(devFile, cnn_vocab, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZNvT-REJgv1"
   },
   "outputs": [],
   "source": [
    "cnn_trainX, cnn_trainY=get_batches(cnn_train_x, cnn_train_y, torch.LongTensor)\n",
    "cnn_devX, cnn_devY=get_batches(cnn_dev_x, cnn_dev_y, torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDGz8mqdJjic"
   },
   "outputs": [],
   "source": [
    "class CNNClassifier(nn.Module):\n",
    "\n",
    "   def __init__(self, params, pretrained_embeddings):\n",
    "      super().__init__()\n",
    "      self.seq_len = params[\"max_seq_len\"]\n",
    "      self.num_labels = params[\"label_length\"]\n",
    "      \n",
    "      '''\n",
    "      Initialize the following layers according to the hw spec\n",
    "      '''\n",
    "      self.embeddings = ...\n",
    "\n",
    "      # convolution over 1 word\n",
    "      self.conv_1 = ...\n",
    "\n",
    "      # convolution over 2 words    \n",
    "      self.conv_2 = ...\n",
    "        \n",
    "      # convolution over 3 words\n",
    "      self.conv_3 = ...\n",
    "        \n",
    "      self.fc = ...\n",
    "\n",
    "    \n",
    "   def forward(self, input): \n",
    "      #embeds the input sequences\n",
    "      x0 = self.embeddings(input)\n",
    "      #changes dimensions to be consistent with conv1d\n",
    "      x0 = x0.permute(0, 2, 1)\n",
    "\n",
    "      '''\n",
    "      Create the hidden representations according to the hw spec\n",
    "      '''\n",
    "\n",
    "      # Apply the one-word convolution, tanh, and max pool\n",
    "      x1 = ...\n",
    "      x1 = ...\n",
    "      x1 = ...\n",
    "\n",
    "      # Apply the two-word convolution, tanh, and max pool\n",
    "      x2 = ...\n",
    "      x2 = ...\n",
    "      x2 = ...\n",
    "\n",
    "      # Apply the three-word convolution, tanh, and max pool\n",
    "      x3 = ...\n",
    "      x3 = ...\n",
    "      x3 = ...\n",
    "\n",
    "      # Concatenates the output of all 3 convolution layers\n",
    "      combined = ...\n",
    "\n",
    "      # Connects the combined output to the fully-connected layer\n",
    "      out = ...\n",
    "      return out.squeeze()\n",
    "\n",
    "\n",
    "   def evaluate(self, x, y):\n",
    "      \n",
    "      self.eval()\n",
    "      corr = 0.\n",
    "      total = 0.\n",
    "\n",
    "      with torch.no_grad():\n",
    "\n",
    "        for x, y in zip(x, y):\n",
    "          y_preds=self.forward(x)\n",
    "          for idx, y_pred in enumerate(y_preds):\n",
    "              prediction=torch.argmax(y_pred)\n",
    "              if prediction == y[idx]:\n",
    "                corr += 1.\n",
    "              total+=1                          \n",
    "      return corr/total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DxrBo0N0JlGM",
    "outputId": "d2b6c84f-84e0-4423-ebd5-825de7266314"
   },
   "outputs": [],
   "source": [
    "# Running this cell should take ~2 minutes.\n",
    "embs, cnn_vocab = read_embeddings(\"glove.6B.50d.50K.txt\")\n",
    "cnnmodel = CNNClassifier(params={\"max_seq_len\": 549, \"label_length\": len(labels)}, pretrained_embeddings=embs)\n",
    "optimizer = torch.optim.Adam(cnnmodel.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "losses = []\n",
    "cross_entropy=nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs=15\n",
    "best_dev_acc = 0.\n",
    "patience=10\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    cnnmodel.train()\n",
    "\n",
    "    for x, y in zip(cnn_trainX, cnn_trainY):\n",
    "      y_pred = cnnmodel.forward(x)\n",
    "      loss = cross_entropy(y_pred.view(-1, cnnmodel.num_labels), y.view(-1))\n",
    "      losses.append(loss) \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    dev_accuracy=cnnmodel.evaluate(cnn_devX, cnn_devY)\n",
    "   \n",
    "    # check if the dev accuracy is the best seen so far; save the model if so\n",
    "    print(\"Epoch %s, dev accuracy: %.3f\" % (epoch, dev_accuracy))\n",
    "    if dev_accuracy > best_dev_acc:\n",
    "      torch.save(cnnmodel.state_dict(), 'best-cnnmodel-parameters.pt')\n",
    "      best_dev_acc = dev_accuracy\n",
    "      patienceCounter=0\n",
    "        \n",
    "    patienceCounter+=1\n",
    "    if patienceCounter >= patience:\n",
    "        print(\"Stopping training; no improvement on dev data after %s epochs\" % patience)\n",
    "        break\n",
    "\n",
    "    \n",
    "cnnmodel.load_state_dict(torch.load('best-cnnmodel-parameters.pt'))\n",
    "print(\"\\nBest Performing Model achieves dev accuracy of : %.3f\" % (best_dev_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8j5kM7T5T69d"
   },
   "source": [
    "# Model Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7u2MerXT9fb"
   },
   "source": [
    "## CNN Loss Examination\n",
    "To debug your model and ensure it is updating correctly, it may be helpful to visualize your training loss.  The following code plots loss over epoch.  This should decrease as the model trains and eventually converge.  If your training loss is not decreasing, you might not be initializing your model or creating your forward() pass correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "0W1WDYkfrdLX",
    "outputId": "bff455eb-85ab-4623-e30c-87c15e517262"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(len(losses)), losses)\n",
    "plt.title(\"Training Loss over Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAW6FujgeUOb"
   },
   "source": [
    "# Deliverable 3. BoW vs. CNN.\n",
    "\n",
    "Compare and contrast the performance of your BoW representation and CNN. Did one model demonstrate a higher dev performance than the other? What do you see as the advantages of one model over the other that might lead to this performance difference on this data? Submit your <200 word answer to this question as a PDF on gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GjQCNmnW_1nh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
